{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Numerax","text":""},{"location":"#numerax","title":"numerax","text":"<p>Statistical and numerical computation functions for JAX, focusing on tools not available in the main JAX API.</p>"},{"location":"#numerax--overview","title":"Overview","text":"<p>This package provides JAX-compatible implementations of specialized numerical functions with full differentiability support. All functions are designed to work seamlessly with JAX's transformations (JIT, grad, vmap, etc.) and follow JAX's functional programming paradigms.</p>"},{"location":"#numerax--special-functions-numeraxspecial","title":"Special Functions (<code>numerax.special</code>)","text":"<p>Mathematical special functions with custom derivative implementations. Functions provide exact gradients through custom JVP rules where standard automatic differentiation would be inefficient or unstable.</p>"},{"location":"#numerax--statistical-methods-numeraxstats","title":"Statistical Methods (<code>numerax.stats</code>)","text":"<p>Advanced statistical computation tools for inference problems. Implements complex statistical models that benefit from JAX's compilation and differentiation capabilities.</p>"},{"location":"#numerax--utilities-numeraxutils","title":"Utilities (<code>numerax.utils</code>)","text":"<p>Development utilities for creating JAX-compatible functions with proper documentation support. Includes decorators and helpers for preserving function metadata when using JAX's advanced features like custom derivatives.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>numerax</code> in your research, please cite it using the citation information from Zenodo (click the badge above) to ensure you get the correct DOI for the version you used.</p>"},{"location":"api/","title":"API Reference","text":"<p>Comprehensive API documentation for all Numerax modules.</p>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>Special Functions - Mathematical special functions with custom derivatives</li> <li>Statistics - Advanced statistical computation tools  </li> <li>Utilities - Development utilities for JAX functions</li> </ul>"},{"location":"api/special/","title":"Special Functions","text":""},{"location":"api/special/#numerax.special","title":"numerax.special","text":""},{"location":"api/special/#numerax.special.erfcinv","title":"erfcinv","text":"<pre><code>erfcinv(x: ArrayLike) -&gt; ArrayLike\n</code></pre> <p>Inverse complementary error function.</p>"},{"location":"api/special/#numerax.special.erfcinv--overview","title":"Overview","text":"<p>Computes the inverse of the complementary error function, finding \\(y\\) such that \\(\\text{erfc}(y) = x\\) for given \\(x \\in (0, 2)\\).</p>"},{"location":"api/special/#numerax.special.erfcinv--mathematical-background","title":"Mathematical Background","text":"<p>The inverse complementary error function is related to the inverse error function by:</p> \\[\\text{erfcinv}(x) = \\text{erfinv}(1 - x)\\] <p>This relationship allows us to implement erfcinv as a simple wrapper around the existing JAX implementation of erfinv.</p>"},{"location":"api/special/#numerax.special.erfcinv--args","title":"Args","text":"<ul> <li>x: Input values in \\((0, 2)\\). Can be scalar or array.</li> </ul>"},{"location":"api/special/#numerax.special.erfcinv--returns","title":"Returns","text":"<p>Values \\(y\\) where \\(\\text{erfc}(y) = x\\).</p>"},{"location":"api/special/#numerax.special.erfcinv--example","title":"Example","text":"<pre><code>import jax.numpy as jnp\nimport numerax\n\n# Single value\ny = numerax.special.erfcinv(0.5)  # \u2248 0.4769\n\n# Array input\nx_vals = jnp.array([0.1, 0.5, 1.0, 1.5, 1.9])\ny_vals = numerax.special.erfcinv(x_vals)\n\n# Differentiable for optimization\ngrad_fn = jax.grad(numerax.special.erfcinv)\nsensitivity = grad_fn(0.5)\n</code></pre>"},{"location":"api/special/#numerax.special.erfcinv--notes","title":"Notes","text":"<ul> <li>Differentiable: Full automatic differentiation support through JAX</li> <li>Broadcasting: Supports JAX array broadcasting</li> <li>Domain: Input must be in \\((0, 2)\\) for real outputs</li> <li>Performance: JIT-compiled compatibility</li> </ul>"},{"location":"api/special/#numerax.special.gammap_inverse","title":"gammap_inverse","text":"<pre><code>gammap_inverse(p: ArrayLike, a: float) -&gt; ArrayLike\n</code></pre> <p>Inverse of the regularized incomplete gamma function.</p>"},{"location":"api/special/#numerax.special.gammap_inverse--overview","title":"Overview","text":"<p>Computes the inverse of the regularized incomplete gamma function, finding \\(x\\) such that \\(P(a, x) = p\\), where \\(P(a, x)\\) is the regularized incomplete gamma function. This is equivalent to computing quantiles of the \\(\\text{Gamma}(a, 1)\\) distribution. The general strategy and the initial guess are based on the methods described in Numerical Recipes (Press et al., 2007).</p>"},{"location":"api/special/#numerax.special.gammap_inverse--mathematical-background","title":"Mathematical Background","text":"<p>The regularized incomplete gamma function is defined as:</p> \\[P(a, x) = \\frac{\\gamma(a, x)}{\\Gamma(a)} = \\frac{1}{\\Gamma(a)} \\int_0^x t^{a-1} e^{-t} dt\\] <p>This function solves the inverse problem:</p> \\[x = P^{-1}(a, p) \\quad \\text{such that} \\quad P(a, x) = p\\] <p>For a random variable \\(X \\sim \\text{Gamma}(a, 1)\\), this gives:</p> \\[x = F^{-1}(p) \\quad \\text{where} \\quad F(x) = P(\\Gamma(a), x)\\]"},{"location":"api/special/#numerax.special.gammap_inverse--numerical-method","title":"Numerical Method","text":"<p>Uses Halley's method for fast quadratic convergence:</p> \\[x_{n+1} = x_n - \\frac{2f(x_n)f'(x_n)}{2[f'(x_n)]^2 - f(x_n)f''(x_n)}\\] <p>where \\(f(x) = P(a, x) - p\\).</p> <p>Initial guess based on Numerical Recipes (Press et al., 2007): - For \\(a &gt; 1\\): Wilson-Hilferty approximation - For \\(a \\leq 1\\): Asymptotic expansions</p>"},{"location":"api/special/#numerax.special.gammap_inverse--args","title":"Args","text":"<ul> <li>p: Probability values in \\([0, 1]\\). Can be scalar or array.</li> <li>a: Shape parameter (must be positive). Scalar value.</li> </ul>"},{"location":"api/special/#numerax.special.gammap_inverse--returns","title":"Returns","text":"<p>Quantiles \\(x\\) where \\(P(a, x) = p\\). Same shape as input <code>p</code>.</p>"},{"location":"api/special/#numerax.special.gammap_inverse--example","title":"Example","text":"<pre><code>import jax.numpy as jnp\nimport numerax\n\n# Single quantile\nx = numerax.special.gammap_inverse(0.5, 2.0)  # Median of Gamma(2, 1)\n\n# Multiple quantiles\np_vals = jnp.array([0.1, 0.25, 0.5, 0.75, 0.9])\nx_vals = numerax.special.gammap_inverse(p_vals, 3.0)\n\n# Verify inverse relationship\nfrom jax.scipy.special import gammainc\n\np_recovered = gammainc(2.0, x)  # Should equal original p\n\n# Differentiable for optimization\ngrad_fn = jax.grad(numerax.special.gammap_inverse)\nsensitivity = grad_fn(0.5, 2.0)  # \u2202x/\u2202p at median\n</code></pre>"},{"location":"api/special/#numerax.special.gammap_inverse--notes","title":"Notes","text":"<ul> <li>Convergence: Typically converges in 3-8 iterations</li> <li>Differentiable: Custom JVP implementation using implicit function   theorem</li> <li>Domain: \\(p \\in [0, 1]\\) and \\(a &gt; 0\\)</li> </ul>"},{"location":"api/special/#numerax.special.gammap_inverse--references","title":"References","text":"<p>Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp; Flannery, B. P. (2007). Numerical Recipes: The Art of Scientific Computing (3rd ed.). Cambridge University Press.</p>"},{"location":"api/stats/","title":"Statistics","text":""},{"location":"api/stats/#numerax.stats","title":"numerax.stats","text":"<p>Statistics submodule for numerax.</p>"},{"location":"api/stats/#numerax.stats.make_profile_llh","title":"make_profile_llh","text":"<pre><code>make_profile_llh(llh_fn: Callable, is_nuisance: list[bool] | ndarray, get_initial_nuisance: Callable, tol: float = 1e-06, initial_value: float = 1e-09, initial_diff: float = 1000000000.0, optimizer: GradientTransformation = _DEFAULT_OPTIMIZER) -&gt; Callable\n</code></pre> <p>Factory function for creating profile likelihood functions.</p>"},{"location":"api/stats/#numerax.stats.make_profile_llh--overview","title":"Overview","text":"<p>Profile likelihood is a statistical technique used when dealing with nuisance parameters that are not of primary interest but are necessary for the model. This function creates an optimized profile likelihood that maximizes over nuisance parameters while keeping inference parameters fixed.</p>"},{"location":"api/stats/#numerax.stats.make_profile_llh--mathematical-background","title":"Mathematical Background","text":"<p>Given a likelihood function \\(L(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda})\\) where \\(\\boldsymbol{\\theta}\\) are parameters of interest and \\(\\boldsymbol{\\lambda}\\) are nuisance parameters, the profile likelihood is:</p> \\[L_p(\\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\lambda}} L(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda})\\] <p>In practice, we work with the log-likelihood \\(\\ell(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = \\log L(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda})\\):</p> \\[\\ell_p(\\boldsymbol{\\theta}) = \\max_{\\boldsymbol{\\lambda}} \\ell(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda})\\] <p>This function uses L-BFGS optimization to find the maximum likelihood estimates of nuisance parameters for each fixed value of inference parameters.</p>"},{"location":"api/stats/#numerax.stats.make_profile_llh--args","title":"Args","text":"<ul> <li>llh_fn: Log likelihood function taking (params, *args) and   returning scalar log likelihood value</li> <li>is_nuisance: Boolean array where True indicates nuisance   parameters and False indicates inference parameters</li> <li>get_initial_nuisance: Function taking (*args) and returning   initial values for nuisance parameters</li> <li>tol: Convergence tolerance for the optimization (default: 1e-6)</li> <li>initial_value: Initial objective value for convergence tracking   (default: 1e-9)</li> <li>initial_diff: Initial difference for convergence tracking   (default: 1e9)</li> <li>optimizer: Optax optimizer to use for maximization   (default: lbfgs()). Currently tested only with the default L-BFGS   optimizer</li> </ul>"},{"location":"api/stats/#numerax.stats.make_profile_llh--returns","title":"Returns","text":"<p>Profile likelihood function with signature: <code>(inference_values, *args) -&gt; (profile_llh_value, optimal_nuisance, convergence_diff, num_iterations)</code></p>"},{"location":"api/stats/#numerax.stats.make_profile_llh--example","title":"Example","text":"<p>Consider fitting a normal distribution where we want to infer the mean \\(\\mu\\) but treat the variance \\(\\sigma^2\\) as a nuisance parameter:</p> <pre><code>import jax.numpy as jnp\nimport numerax\n\n# Sample data\ndata = jnp.array([1.2, 0.8, 1.5, 0.9, 1.1, 1.3, 0.7, 1.4])\n\n\n# Log likelihood for normal distribution\ndef normal_llh(params, data):\n    mu, log_sigma = params  # Use log(sigma) for numerical stability\n    sigma = jnp.exp(log_sigma)\n    return jnp.sum(\n        -0.5 * jnp.log(2 * jnp.pi)\n        - log_sigma\n        - 0.5 * ((data - mu) / sigma) ** 2\n    )\n\n\n# Profile over log_sigma (nuisance), infer mu\nis_nuisance = [False, True]  # mu=inference, log_sigma=nuisance\n\n\ndef get_initial_log_sigma(data):\n    # Initialize with log of sample standard deviation\n    return jnp.array([jnp.log(jnp.std(data))])\n\n\nprofile_llh = numerax.stats.make_profile_llh(\n    normal_llh, is_nuisance, get_initial_log_sigma\n)\n\n# Evaluate profile likelihood at different mu values\nmu_test = 1.0\nllh_val, opt_log_sigma, diff, n_iter = profile_llh(\n    jnp.array([mu_test]), data\n)\n</code></pre>"},{"location":"api/stats/#numerax.stats.make_profile_llh--notes","title":"Notes","text":"<ul> <li>The function is JIT-compiled for performance</li> <li>Uses L-BFGS optimization which is well-suited for smooth likelihood   surfaces</li> <li>Returns convergence information for diagnostics</li> <li>Handles parameter masking automatically</li> <li>Consider using log-parameterization for positive parameters   (e.g., \\(\\log \\sigma\\)) for unconstrained optimization</li> <li>This function might not work well if the likelihood surface has   multiple local maxima; in such cases, consider ensuring that   initial guesses are close to the global maximum.</li> </ul>"},{"location":"api/stats/#numerax.stats.chi2","title":"chi2","text":"<p>Chi-squared distribution functions.</p> <p>This module provides a complete interface for chi-squared distribution computations, combining re-exported JAX's standard statistical functions (pdf, cdf, etc.) with a custom high-precision percent point function (ppf).</p> <p>All functions support location-scale parameterization and are fully compatible with JAX transformations (JIT, grad, vmap).</p>"},{"location":"api/stats/#numerax.stats.chi2.ppf","title":"ppf","text":"<pre><code>ppf(q: ArrayLike, df: ArrayLike, loc: ArrayLike = 0, scale: ArrayLike = 1) -&gt; ArrayLike\n</code></pre> <p>Chi-squared percent point function (inverse CDF).</p>"},{"location":"api/stats/#numerax.stats.chi2.ppf--overview","title":"Overview","text":"<p>Computes the percent point function (quantile function) of the chi-squared distribution. This is the inverse of the cumulative distribution function, finding \\(x\\) such that \\(P(X \\leq x) = q\\) for a chi-squared random variable \\(X\\) with \\(\\text{df}\\) degrees of freedom.</p>"},{"location":"api/stats/#numerax.stats.chi2.ppf--mathematical-background","title":"Mathematical Background","text":"<p>The chi-squared distribution with \\(\\text{df}\\) degrees of freedom is a special case of the gamma distribution:</p> \\[X \\sim \\chi^2(\\text{df}) \\equiv \\text{Gamma}\\left( \\frac{\\text{df}}{2}, 2\\right)\\] <p>For the location-scale family:</p> \\[Y = \\text{loc} + \\text{scale} \\cdot X\\] <p>The percent point function is computed as:</p> \\[ \\text{ppf}(q, \\text{df}, \\text{loc}, \\text{scale}) = \\text{loc} + \\text{scale} \\cdot 2 \\cdot \\text{gammap\\_inverse}\\left(q, \\frac{\\text{df}}{2}\\right) \\]"},{"location":"api/stats/#numerax.stats.chi2.ppf--args","title":"Args","text":"<ul> <li>q: Probability values in \\([0, 1]\\). Can be scalar or array.</li> <li>df: Degrees of freedom (must be positive). Can be scalar or array.</li> <li>loc: Location parameter (default: 0). Can be scalar or array.</li> <li>scale: Scale parameter (must be positive, default: 1). Can be   scalar or array.</li> </ul>"},{"location":"api/stats/#numerax.stats.chi2.ppf--returns","title":"Returns","text":"<p>Quantiles \\(x\\) where \\(P(X \\leq x) = q\\). Shape follows JAX broadcasting rules.</p>"},{"location":"api/stats/#numerax.stats.chi2.ppf--example","title":"Example","text":"<pre><code>import jax.numpy as jnp\nimport numerax\n\n# Single quantile\nx = numerax.stats.chi2.ppf(0.5, df=2)  # Median of \u03c7\u00b2(2)\n\n# Multiple quantiles\nq_vals = jnp.array([0.1, 0.25, 0.5, 0.75, 0.9])\nx_vals = numerax.stats.chi2.ppf(q_vals, df=3)\n\n# Location-scale family\nx_scaled = numerax.stats.chi2.ppf(0.5, df=2, loc=1, scale=2)\n\n# Differentiable for optimization\ngrad_fn = jax.grad(numerax.stats.chi2.ppf)\nsensitivity = grad_fn(0.5, 2.0)  # \u2202x/\u2202q at median\n</code></pre>"},{"location":"api/stats/#numerax.stats.chi2.ppf--notes","title":"Notes","text":"<ul> <li>Differentiable: Automatic differentiation through <code>gammap_inverse</code></li> <li>Broadcasting: Supports JAX array broadcasting for all parameters</li> <li>Performance: JIT-compiled compatibility</li> </ul>"},{"location":"api/utils/","title":"Utilities","text":""},{"location":"api/utils/#numerax.utils","title":"numerax.utils","text":"<p>Utility functions for the numerax package.</p> <p>This module provides development utilities for creating JAX-compatible functions and tools for working with PyTree structures, including parameter counting for machine learning models.</p>"},{"location":"api/utils/#numerax.utils.count_params","title":"count_params","text":"<pre><code>count_params(pytree, filter=None, verbose=True)\n</code></pre> <p>Count the total number of parameters in a PyTree structure.</p>"},{"location":"api/utils/#numerax.utils.count_params--overview","title":"Overview","text":"<p>This function counts parameters in PyTree-based models by filtering for array-like objects and summing their sizes. It is particularly useful for neural network models built with JAX frameworks like Equinox.</p> <p>The function traverses the PyTree structure, applies a filter to identify parameter arrays, and computes the total parameter count.</p>"},{"location":"api/utils/#numerax.utils.count_params--args","title":"Args","text":"<ul> <li>pytree: The PyTree structure to count parameters in (e.g., a   model, dict of arrays, or nested structure)</li> <li>filter: Optional filter function to identify parameters. If   <code>None</code>, uses <code>equinox.is_array</code> as the default filter. Custom   filters should accept a single argument and return <code>True</code> for   objects that should be counted</li> <li>verbose: If <code>True</code>, prints the parameter count in scientific   notation. If <code>False</code>, only returns the count silently</li> </ul>"},{"location":"api/utils/#numerax.utils.count_params--returns","title":"Returns","text":"<p>The total number of parameters as an integer</p>"},{"location":"api/utils/#numerax.utils.count_params--requirements","title":"Requirements","text":"<ul> <li>equinox: Install with <code>pip install numerax[sciml]</code> or   <code>pip install equinox</code></li> </ul>"},{"location":"api/utils/#numerax.utils.count_params--example","title":"Example","text":"<pre><code>import jax.numpy as jnp\nfrom numerax.utils import count_params\n\n# Simple dict-based model\nmodel = {\"weights\": jnp.ones((10, 5)), \"bias\": jnp.zeros(5)}\ncount = count_params(model)\n# Prints: Number of parameters: 5.5e+01\n# Returns: 55\n\n# With custom filter\ncount = count_params(\n    model,\n    filter=lambda x: hasattr(x, \"ndim\") and x.ndim &gt; 1,\n    verbose=False,\n)\n# Returns: 50 (only the weights matrix)\n\n# With Equinox model\nimport equinox as eqx\n\n\nclass MLP(eqx.Module):\n    layers: list\n\n    def __init__(self, key):\n        self.layers = [\n            eqx.nn.Linear(10, 64, key=key),\n            eqx.nn.Linear(64, 1, key=key),\n        ]\n\n\nmodel = MLP(jax.random.PRNGKey(0))\ncount = count_params(model)\n# Counts all trainable parameters in the MLP\n</code></pre>"},{"location":"api/utils/#numerax.utils.count_params--notes","title":"Notes","text":"<ul> <li>The default filter (<code>equinox.is_array</code>) correctly identifies   parameter arrays in Equinox modules and standard JAX PyTrees</li> <li>For custom filtering logic, provide a function that returns   <code>True</code> for leaves that should be counted as parameters</li> <li>The function handles nested PyTree structures automatically</li> </ul>"},{"location":"api/utils/#numerax.utils.preserve_metadata","title":"preserve_metadata","text":"<pre><code>preserve_metadata(decorator)\n</code></pre> <p>Wrapper that ensures a decorator preserves function metadata for documentation tools.</p>"},{"location":"api/utils/#numerax.utils.preserve_metadata--overview","title":"Overview","text":"<p>This is particularly useful for JAX decorators like <code>@custom_jvp</code> that create special objects which may not preserve <code>__doc__</code> and other metadata properly for documentation generators like pdoc.</p>"},{"location":"api/utils/#numerax.utils.preserve_metadata--args","title":"Args","text":"<ul> <li>decorator: The decorator function to wrap</li> </ul>"},{"location":"api/utils/#numerax.utils.preserve_metadata--returns","title":"Returns","text":"<p>A new decorator that preserves metadata</p>"},{"location":"api/utils/#numerax.utils.preserve_metadata--example","title":"Example","text":"<pre><code>import jax\nfrom numerax.utils import preserve_metadata\n\n@preserve_metadata(jax.custom_jvp)\ndef my_function(x):\n    \"\"\"This docstring will be preserved for automatic\n    documentation generation.\"\"\"\n    return x\n</code></pre>"},{"location":"api/utils/#numerax.utils.tree_summary","title":"tree_summary","text":"<pre><code>tree_summary(pytree, is_leaf=None, max_depth=3, verbose=True, hide_empty=True)\n</code></pre> <p>Pretty-print PyTree structure with shapes and parameter counts.</p>"},{"location":"api/utils/#numerax.utils.tree_summary--overview","title":"Overview","text":"<p>This function displays a hierarchical view of a PyTree structure (e.g., neural network models) showing the organization, array shapes, data types, and parameter counts at each level. The output is similar to Keras' <code>model.summary()</code> or torchinfo's summaries. This is compatible with PyTree-based models from frameworks like Equinox.</p>"},{"location":"api/utils/#numerax.utils.tree_summary--args","title":"Args","text":"<ul> <li>pytree: The PyTree structure to summarize (e.g., a model,   dict of arrays, or nested structure)</li> <li>is_leaf: Optional function to identify leaf nodes. If <code>None</code>,   uses <code>equinox.is_array</code> as the default. Leaf nodes are displayed   with shape, dtype, and parameter count details. Custom functions   should accept a single argument and return <code>True</code> for leaves</li> <li>max_depth: Maximum nesting depth to display. Nodes deeper   than this level will not be shown. Defaults to 3</li> <li>verbose: If <code>True</code>, prints the formatted summary. If <code>False</code>,   only returns the total parameter count silently</li> <li>hide_empty: If <code>True</code>, skips nodes with zero parameters.   Defaults to <code>True</code> to avoid clutter from primitive attributes   (integers, strings, functions) in neural network modules that   don't contribute to parameter counts</li> </ul>"},{"location":"api/utils/#numerax.utils.tree_summary--returns","title":"Returns","text":"<p>The total number of parameters as an integer</p>"},{"location":"api/utils/#numerax.utils.tree_summary--requirements","title":"Requirements","text":"<ul> <li>equinox: Install with <code>pip install numerax[sciml]</code> or   <code>pip install equinox</code> (required when using default <code>is_leaf</code>)</li> </ul>"},{"location":"api/utils/#numerax.utils.tree_summary--example","title":"Example","text":"<pre><code>import jax.numpy as jnp\nfrom numerax.utils import tree_summary\n\n# Nested dict-based model\nmodel = {\n    \"encoder\": {\n        \"weights\": jnp.ones((10, 20)),\n        \"bias\": jnp.zeros(20),\n    },\n    \"decoder\": {\n        \"weights\": jnp.ones((20, 5)),\n        \"bias\": jnp.zeros(5),\n    },\n}\n\ncount = tree_summary(model)\n# Prints formatted table showing structure\n# Returns: 325\n\n# With custom is_leaf function\ntree_summary(model, is_leaf=lambda x: hasattr(x, \"shape\"))\n\n# Limit depth\ntree_summary(model, max_depth=2)\n\n# Silent mode\ncount = tree_summary(model, verbose=False)\n# Returns: 325 without printing\n</code></pre>"},{"location":"api/utils/#numerax.utils.tree_summary--output-format","title":"Output Format","text":"<pre><code>======================================================================\nPyTree Summary\n======================================================================\nName                  Shape           Dtype             Params\n----------------------------------------------------------------------\nroot                                                       325\n  encoder                                                  220\n    - weights         [10,20]         float32              200\n    - bias            [20]            float32               20\n  decoder                                                  105\n    - weights         [20,5]          float32              100\n    - bias            [5]             float32                5\n======================================================================\nTotal params: 325\n======================================================================\n</code></pre>"},{"location":"api/utils/#numerax.utils.tree_summary--notes","title":"Notes","text":"<ul> <li>Container nodes (dicts, lists, modules) show total parameter   counts for their entire subtree</li> <li>Leaf nodes (arrays) show shape, dtype, and individual param count</li> <li>Indentation shows nesting depth in the PyTree structure</li> <li>Works with Equinox modules, nested dicts, lists, tuples, and   custom PyTree nodes</li> <li>Use custom <code>is_leaf</code> functions to control what counts as a leaf   node (useful for custom PyTree registrations)</li> </ul>"}]}